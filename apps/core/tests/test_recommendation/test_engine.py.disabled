"""
Unit tests for recommendation engine components
"""
import pytest
import numpy as np
from unittest.mock import patch, MagicMock, Mock
from django.utils import timezone
from datetime import timedelta
import random

from apps.core.recommendation_engine import (
    RecommendationEngine, CollaborativeFilteringEngine, ContentBasedEngine,
    NavigationAnalyzer, BehaviorAnalyzer, MultiArmedBandit
)
from apps.core.models.recommendation import (
    UserBehaviorProfile, NavigationRecommendation, ContentRecommendation, UserSimilarity
)
from apps.core.models.heatmap import HeatmapSession, ClickHeatmap, ScrollHeatmap
from tests.factories.recommendation_factories import (
    UserBehaviorProfileFactory, NavigationRecommendationFactory, ContentRecommendationFactory,
    UserSimilarityFactory, create_recommendation_scenario
)
from tests.factories.heatmap_factories import (
    UserFactory, HeatmapSessionFactory, ClickHeatmapFactory, ScrollHeatmapFactory
)

pytestmark = pytest.mark.django_db


class TestRecommendationEngine:
    """Test main RecommendationEngine class"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.engine = RecommendationEngine()
        self.user = UserFactory()
        self.profile = UserBehaviorProfileFactory(user=self.user)
    
    def test_engine_initialization(self):
        """Test that engine initializes with all components"""
        assert isinstance(self.engine.collaborative_engine, CollaborativeFilteringEngine)
        assert isinstance(self.engine.content_engine, ContentBasedEngine)
        assert isinstance(self.engine.navigation_analyzer, NavigationAnalyzer)
        assert isinstance(self.engine.behavior_analyzer, BehaviorAnalyzer)
        assert isinstance(self.engine.bandit, MultiArmedBandit)
    
    def test_generate_user_recommendations(self):
        """Test generating recommendations for a user"""
        # Create some existing recommendations to test with
        for _ in range(3):
            ContentRecommendationFactory(user=self.user)
        
        with patch.object(self.engine.collaborative_engine, 'generate_recommendations') as mock_collab:
            with patch.object(self.engine.content_engine, 'generate_recommendations') as mock_content:
                # Mock return values
                mock_collab.return_value = [ContentRecommendationFactory.build(user=self.user)]
                mock_content.return_value = [ContentRecommendationFactory.build(user=self.user)]
                
                recommendations = self.engine.generate_user_recommendations(self.user, limit=5)
                
                assert len(recommendations) <= 5
                mock_collab.assert_called_once_with(self.user, limit=3)
                mock_content.assert_called_once_with(self.user, limit=2)
    
    def test_generate_user_recommendations_with_bandit_selection(self):
        """Test recommendation generation with bandit algorithm selection"""
        with patch.object(self.engine.bandit, 'select_algorithm') as mock_select:
            mock_select.return_value = 'collaborative_filtering'
            
            with patch.object(self.engine.collaborative_engine, 'generate_recommendations') as mock_collab:
                mock_collab.return_value = []
                
                self.engine.generate_user_recommendations(self.user, use_bandit=True)
                
                mock_select.assert_called_once()
                mock_collab.assert_called_once()
    
    def test_generate_navigation_recommendations(self):
        """Test generating navigation recommendations"""
        with patch.object(self.engine.navigation_analyzer, 'analyze_navigation_patterns') as mock_analyze:
            mock_analyze.return_value = [NavigationRecommendationFactory.build()]
            
            recommendations = self.engine.generate_navigation_recommendations()
            
            assert len(recommendations) >= 0
            mock_analyze.assert_called_once()
    
    def test_update_user_behavior(self):
        """Test updating user behavior data"""
        session_data = {
            'page_url': '/test-page/',
            'user_agent': 'Test Agent',
            'timestamp': timezone.now(),
            'device_type': 'desktop'
        }
        
        with patch.object(self.engine.behavior_analyzer, 'update_user_profile') as mock_update:
            self.engine.update_user_behavior(self.user, session_data)
            
            mock_update.assert_called_once_with(self.user, session_data)
    
    def test_recommendation_algorithm_mixing(self):
        """Test that recommendations are properly mixed from different algorithms"""
        # Mock different engines to return different recommendations
        collab_recs = [ContentRecommendationFactory.build(
            user=self.user, 
            recommendation_algorithm='collaborative_filtering',
            relevance_score=0.9
        )]
        content_recs = [ContentRecommendationFactory.build(
            user=self.user,
            recommendation_algorithm='content_based',
            relevance_score=0.8
        )]
        
        with patch.object(self.engine.collaborative_engine, 'generate_recommendations', return_value=collab_recs):
            with patch.object(self.engine.content_engine, 'generate_recommendations', return_value=content_recs):
                recommendations = self.engine.generate_user_recommendations(self.user, limit=10)
                
                # Should contain both types
                algorithms = [rec.recommendation_algorithm for rec in recommendations]
                assert 'collaborative_filtering' in algorithms
                assert 'content_based' in algorithms
    
    def test_recommendation_deduplication(self):
        """Test that duplicate recommendations are filtered out"""
        duplicate_url = '/duplicate-page/'
        
        # Create recommendations with same URL from different algorithms
        collab_rec = ContentRecommendationFactory.build(
            user=self.user,
            content_url=duplicate_url,
            recommendation_algorithm='collaborative_filtering'
        )
        content_rec = ContentRecommendationFactory.build(
            user=self.user,
            content_url=duplicate_url,
            recommendation_algorithm='content_based'
        )
        
        with patch.object(self.engine.collaborative_engine, 'generate_recommendations', return_value=[collab_rec]):
            with patch.object(self.engine.content_engine, 'generate_recommendations', return_value=[content_rec]):
                recommendations = self.engine.generate_user_recommendations(self.user)
                
                # Should only contain one recommendation for the URL
                urls = [rec.content_url for rec in recommendations]
                assert urls.count(duplicate_url) == 1


class TestCollaborativeFilteringEngine:
    """Test CollaborativeFilteringEngine"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.engine = CollaborativeFilteringEngine()
        self.user = UserFactory()
        self.profile = UserBehaviorProfileFactory(user=self.user)
    
    def test_engine_initialization(self):
        """Test engine initialization"""
        assert self.engine.min_similarity_score == 0.3
        assert self.engine.max_similar_users == 10
    
    def test_calculate_user_similarities(self):
        """Test calculating similarities between users"""
        # Create other users with profiles
        other_users = []
        for i in range(3):
            other_user = UserFactory()
            other_profile = UserBehaviorProfileFactory(
                user=other_user,
                similarity_vector=[0.1 * i, 0.2 * i, 0.3 * i, 0.4 * i, 0.5 * i]
            )
            other_users.append(other_user)
        
        # Set a specific vector for our test user
        self.profile.similarity_vector = [0.5, 0.4, 0.3, 0.2, 0.1]
        self.profile.save()
        
        with patch('apps.core.recommendation_engine.CollaborativeFilteringEngine._calculate_cosine_similarity') as mock_cosine:
            mock_cosine.side_effect = [0.8, 0.6, 0.4]  # Different similarities
            
            self.engine.calculate_user_similarities(self.user)
            
            # Should create similarity records
            similarities = UserSimilarity.objects.filter(user1=self.user)
            assert similarities.count() == 3
            assert mock_cosine.call_count == 3
    
    def test_cosine_similarity_calculation(self):
        """Test cosine similarity calculation"""
        profile1 = UserBehaviorProfileFactory(similarity_vector=[1, 0, 0])
        profile2 = UserBehaviorProfileFactory(similarity_vector=[0, 1, 0])
        profile3 = UserBehaviorProfileFactory(similarity_vector=[1, 0, 0])
        
        # Perpendicular vectors should have similarity 0
        sim_perpendicular = self.engine._calculate_cosine_similarity(profile1, profile2)
        assert abs(sim_perpendicular - 0.0) < 0.01
        
        # Identical vectors should have similarity 1
        sim_identical = self.engine._calculate_cosine_similarity(profile1, profile3)
        assert abs(sim_identical - 1.0) < 0.01
    
    def test_cosine_similarity_edge_cases(self):
        """Test cosine similarity with edge cases"""
        # Zero vectors
        profile_zero1 = UserBehaviorProfileFactory(similarity_vector=[0, 0, 0])
        profile_zero2 = UserBehaviorProfileFactory(similarity_vector=[0, 0, 0])
        
        sim_zeros = self.engine._calculate_cosine_similarity(profile_zero1, profile_zero2)
        assert sim_zeros == 0.0
        
        # One zero vector
        profile_normal = UserBehaviorProfileFactory(similarity_vector=[1, 2, 3])
        sim_mixed = self.engine._calculate_cosine_similarity(profile_zero1, profile_normal)
        assert sim_mixed == 0.0
    
    def test_get_similar_users(self):
        """Test getting similar users"""
        # Create similar users
        similar_users = []
        for i in range(5):
            similar_user = UserFactory()
            UserSimilarityFactory(
                user1=self.user,
                user2=similar_user,
                similarity_score=0.9 - i * 0.1  # 0.9, 0.8, 0.7, 0.6, 0.5
            )
            similar_users.append(similar_user)
        
        # Get top 3 similar users
        result = self.engine._get_similar_users(self.user, limit=3, min_score=0.6)
        
        assert len(result) == 3
        # Should be ordered by similarity score
        scores = [sim.similarity_score for sim in result]
        assert scores == sorted(scores, reverse=True)
        assert all(score >= 0.6 for score in scores)
    
    def test_generate_recommendations(self):
        """Test generating collaborative filtering recommendations"""
        # Create similar users with recommendations
        similar_user = UserFactory()
        UserSimilarityFactory(user1=self.user, user2=similar_user, similarity_score=0.8)
        
        # Similar user has some recommendations
        similar_rec = ContentRecommendationFactory(
            user=similar_user,
            content_url='/recommended-page/',
            relevance_score=0.9
        )
        
        with patch.object(self.engine, '_get_similar_users') as mock_similar:
            mock_similar.return_value = [
                UserSimilarity(user1=self.user, user2=similar_user, similarity_score=0.8)
            ]
            
            recommendations = self.engine.generate_recommendations(self.user, limit=5)
            
            assert len(recommendations) > 0
            # Should include recommendations from similar users
            urls = [rec.content_url for rec in recommendations]
            # Note: The exact URL might not be in results due to filtering logic
    
    def test_filter_existing_recommendations(self):
        """Test filtering out recommendations user already has"""
        # Create existing recommendation for user
        existing_rec = ContentRecommendationFactory(
            user=self.user,
            content_url='/existing-page/'
        )
        
        # Create similar user with same recommendation
        similar_user = UserFactory()
        ContentRecommendationFactory(
            user=similar_user,
            content_url='/existing-page/'
        )
        
        UserSimilarityFactory(user1=self.user, user2=similar_user, similarity_score=0.8)
        
        recommendations = self.engine.generate_recommendations(self.user)
        
        # Should not include existing recommendation
        urls = [rec.content_url for rec in recommendations]
        assert '/existing-page/' not in urls


class TestContentBasedEngine:
    """Test ContentBasedEngine"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.engine = ContentBasedEngine()
        self.user = UserFactory()
        self.profile = UserBehaviorProfileFactory(user=self.user)
    
    def test_generate_recommendations(self):
        """Test generating content-based recommendations"""
        # Set user preferences
        self.profile.preferred_content_types = ['dashboard', 'report']
        self.profile.preferred_pages = {
            '/dashboard/main/': 50,
            '/reports/monthly/': 30
        }
        self.profile.save()
        
        with patch.object(self.engine, '_find_similar_content') as mock_similar:
            mock_similar.return_value = [
                ContentRecommendationFactory.build(
                    user=self.user,
                    content_type='dashboard',
                    relevance_score=0.85
                )
            ]
            
            recommendations = self.engine.generate_recommendations(self.user, limit=5)
            
            assert len(recommendations) > 0
            mock_similar.assert_called_once()
    
    def test_extract_content_features(self):
        """Test content feature extraction"""
        content_data = {
            'url': '/reports/sales/',
            'title': 'Sales Report Dashboard',
            'content_type': 'report',
            'tags': ['sales', 'analytics', 'dashboard']
        }
        
        features = self.engine._extract_content_features(content_data)
        
        assert isinstance(features, dict)
        assert 'content_type' in features
        assert 'url_pattern' in features
        assert 'title_keywords' in features
    
    def test_calculate_content_similarity(self):
        """Test content similarity calculation"""
        content1 = {
            'content_type': 'report',
            'url_pattern': 'sales',
            'title_keywords': ['sales', 'revenue']
        }
        content2 = {
            'content_type': 'report',
            'url_pattern': 'sales',
            'title_keywords': ['sales', 'profit']
        }
        content3 = {
            'content_type': 'dashboard',
            'url_pattern': 'user',
            'title_keywords': ['users', 'management']
        }
        
        # Similar content should have higher similarity
        sim_similar = self.engine._calculate_content_similarity(content1, content2)
        sim_different = self.engine._calculate_content_similarity(content1, content3)
        
        assert sim_similar > sim_different
        assert 0 <= sim_similar <= 1
        assert 0 <= sim_different <= 1
    
    def test_find_similar_content(self):
        """Test finding content similar to user preferences"""
        # Mock user's preferred content
        user_preferences = {
            'content_types': ['report', 'dashboard'],
            'url_patterns': ['sales', 'revenue'],
            'keywords': ['analytics', 'data']
        }
        
        with patch.object(self.engine, '_get_available_content') as mock_content:
            with patch.object(self.engine, '_calculate_content_similarity') as mock_similarity:
                mock_content.return_value = [
                    {'url': '/reports/sales/', 'content_type': 'report'},
                    {'url': '/dashboard/analytics/', 'content_type': 'dashboard'}
                ]
                mock_similarity.side_effect = [0.9, 0.7]
                
                similar_content = self.engine._find_similar_content(user_preferences, limit=5)
                
                assert len(similar_content) == 2
                # Should be ordered by similarity
                assert similar_content[0]['similarity_score'] >= similar_content[1]['similarity_score']


class TestNavigationAnalyzer:
    """Test NavigationAnalyzer"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.analyzer = NavigationAnalyzer()
    
    def test_analyze_navigation_patterns(self):
        """Test analyzing navigation patterns"""
        # Create some heatmap sessions with navigation data
        for i in range(10):
            session = HeatmapSessionFactory(page_url=f'/page{i%3}/')  # 3 different pages
            ClickHeatmapFactory(session=session, is_navigation=True)
            ScrollHeatmapFactory(session=session)
        
        with patch.object(self.analyzer, '_detect_bottlenecks') as mock_bottlenecks:
            with patch.object(self.analyzer, '_suggest_improvements') as mock_improvements:
                mock_bottlenecks.return_value = ['/page1/']
                mock_improvements.return_value = [NavigationRecommendationFactory.build()]
                
                recommendations = self.analyzer.analyze_navigation_patterns()
                
                assert len(recommendations) > 0
                mock_bottlenecks.assert_called_once()
                mock_improvements.assert_called_once()
    
    def test_detect_bottlenecks(self):
        """Test detecting navigation bottlenecks"""
        # Create sessions with high bounce rate on specific page
        problem_page = '/problem-page/'
        
        # High bounce rate page
        for i in range(20):
            session = HeatmapSessionFactory(page_url=problem_page, is_active=False)
            session.duration_seconds = 5  # Very short session
            session.save()
        
        # Normal pages
        for i in range(10):
            session = HeatmapSessionFactory(page_url='/normal-page/', is_active=False)
            session.duration_seconds = 120  # Normal session
            session.save()
        
        bottlenecks = self.analyzer._detect_bottlenecks()
        
        # Should identify the problem page
        assert problem_page in bottlenecks
    
    def test_suggest_improvements(self):
        """Test suggesting navigation improvements"""
        bottlenecks = ['/slow-page/', '/confusing-page/']
        
        with patch.object(self.analyzer, '_analyze_page_performance') as mock_performance:
            mock_performance.return_value = {
                'avg_time_on_page': 5.0,
                'bounce_rate': 0.8,
                'common_exit_points': ['header', 'sidebar']
            }
            
            improvements = self.analyzer._suggest_improvements(bottlenecks)
            
            assert len(improvements) > 0
            # Should suggest improvements for each bottleneck
            assert len(improvements) >= len(bottlenecks)
    
    def test_calculate_page_metrics(self):
        """Test calculating page-specific metrics"""
        page_url = '/test-metrics-page/'
        
        # Create sessions with various durations
        durations = [10, 30, 45, 60, 90]  # seconds
        for duration in durations:
            session = HeatmapSessionFactory(page_url=page_url, is_active=False)
            session.duration_seconds = duration
            session.save()
        
        metrics = self.analyzer._calculate_page_metrics(page_url)
        
        assert 'avg_duration' in metrics
        assert 'bounce_rate' in metrics
        assert 'total_sessions' in metrics
        assert metrics['total_sessions'] == len(durations)
        assert metrics['avg_duration'] == sum(durations) / len(durations)


class TestBehaviorAnalyzer:
    """Test BehaviorAnalyzer"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.analyzer = BehaviorAnalyzer()
        self.user = UserFactory()
    
    def test_build_user_profile(self):
        """Test building user behavior profile"""
        # Create heatmap sessions for user
        sessions = []
        for i in range(5):
            session = HeatmapSessionFactory(
                user=self.user,
                page_url=f'/page{i%3}/',  # Mix of 3 different pages
                device_type=['desktop', 'mobile'][i%2]
            )
            sessions.append(session)
            
            # Add some clicks and scrolls
            ClickHeatmapFactory(session=session)
            ScrollHeatmapFactory(session=session)
        
        profile = self.analyzer.build_user_profile(self.user)
        
        assert isinstance(profile, UserBehaviorProfile)
        assert profile.user == self.user
        assert len(profile.preferred_pages) > 0
        assert len(profile.similarity_vector) > 0
        assert profile.preferred_device_type in ['desktop', 'mobile', 'tablet']
    
    def test_update_user_profile(self):
        """Test updating existing user profile"""
        # Create existing profile
        profile = UserBehaviorProfileFactory(
            user=self.user,
            preferred_pages={'/old-page/': 10}
        )
        
        # New session data
        session_data = {
            'page_url': '/new-page/',
            'device_type': 'mobile',
            'timestamp': timezone.now(),
            'user_agent': 'Mobile Agent'
        }
        
        self.analyzer.update_user_profile(self.user, session_data)
        
        profile.refresh_from_db()
        assert '/new-page/' in profile.preferred_pages
    
    def test_calculate_feature_vector(self):
        """Test calculating feature vector for similarity"""
        # Create user sessions with specific patterns
        profile = UserBehaviorProfileFactory(
            user=self.user,
            preferred_pages={'/dashboard/': 40, '/reports/': 30, '/settings/': 5},
            preferred_device_type='desktop',
            exploration_tendency=0.7,
            task_completion_rate=0.8
        )
        
        vector = self.analyzer.calculate_feature_vector(self.user)
        
        assert isinstance(vector, list)
        assert len(vector) > 0
        assert all(isinstance(v, (int, float)) for v in vector)
        # Values should be normalized
        assert all(0 <= v <= 1 for v in vector if v >= 0)
    
    def test_detect_user_patterns(self):
        """Test detecting user behavior patterns"""
        # Create sessions with specific patterns
        for i in range(10):
            session = HeatmapSessionFactory(
                user=self.user,
                page_url='/dashboard/' if i < 7 else '/reports/',
                start_time=timezone.now() - timedelta(days=i)
            )
            
            # Morning sessions (9 AM)
            if i % 2 == 0:
                session.start_time = session.start_time.replace(hour=9)
            else:
                session.start_time = session.start_time.replace(hour=15)
            session.save()
        
        patterns = self.analyzer._detect_user_patterns(self.user)
        
        assert 'page_preferences' in patterns
        assert 'time_patterns' in patterns
        assert 'device_preferences' in patterns
        
        # Should detect dashboard preference
        assert patterns['page_preferences']['/dashboard/'] > patterns['page_preferences'].get('/reports/', 0)


class TestMultiArmedBandit:
    """Test MultiArmedBandit for algorithm selection"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.bandit = MultiArmedBandit()
        self.algorithms = ['collaborative_filtering', 'content_based', 'hybrid']
    
    def test_bandit_initialization(self):
        """Test bandit initialization"""
        assert self.bandit.epsilon == 0.1
        assert self.bandit.alpha == 1.0
        assert self.bandit.beta == 1.0
        assert isinstance(self.bandit.algorithm_stats, dict)
    
    def test_thompson_sampling_selection(self):
        """Test Thompson Sampling algorithm selection"""
        with patch('random.betavariate') as mock_beta:
            mock_beta.side_effect = [0.8, 0.6, 0.9]  # Returns for each algorithm
            
            selected = self.bandit.select_algorithm(method='thompson_sampling')
            
            assert selected in self.algorithms
            # Should select the algorithm with highest beta sample (0.9)
    
    def test_ucb1_selection(self):
        """Test UCB1 algorithm selection"""
        # Initialize some stats
        self.bandit.algorithm_stats = {
            'collaborative_filtering': {'successes': 10, 'trials': 20},
            'content_based': {'successes': 5, 'trials': 10},
            'hybrid': {'successes': 1, 'trials': 2}
        }
        
        selected = self.bandit.select_algorithm(method='ucb1')
        assert selected in self.algorithms
    
    def test_epsilon_greedy_selection(self):
        """Test Epsilon-Greedy algorithm selection"""
        # Set up algorithm performance data
        self.bandit.algorithm_stats = {
            'collaborative_filtering': {'successes': 15, 'trials': 20},  # 75% success
            'content_based': {'successes': 8, 'trials': 20},           # 40% success
            'hybrid': {'successes': 12, 'trials': 20}                  # 60% success
        }
        
        # Test exploitation (should select best performing)
        with patch('random.random', return_value=0.5):  # > epsilon, so exploit
            selected = self.bandit.select_algorithm(method='epsilon_greedy')
            assert selected == 'collaborative_filtering'  # Best performer
        
        # Test exploration (should select randomly)
        with patch('random.random', return_value=0.05):  # < epsilon, so explore
            with patch('random.choice', return_value='content_based') as mock_choice:
                selected = self.bandit.select_algorithm(method='epsilon_greedy')
                assert selected == 'content_based'
                mock_choice.assert_called_once_with(self.algorithms)
    
    def test_update_algorithm_performance(self):
        """Test updating algorithm performance stats"""
        algorithm = 'collaborative_filtering'
        
        # Update with success
        self.bandit.update_algorithm_performance(algorithm, success=True)
        stats = self.bandit.algorithm_stats[algorithm]
        assert stats['successes'] == 1
        assert stats['trials'] == 1
        
        # Update with failure
        self.bandit.update_algorithm_performance(algorithm, success=False)
        stats = self.bandit.algorithm_stats[algorithm]
        assert stats['successes'] == 1
        assert stats['trials'] == 2
    
    def test_get_algorithm_performance(self):
        """Test getting algorithm performance metrics"""
        algorithm = 'collaborative_filtering'
        self.bandit.algorithm_stats[algorithm] = {'successes': 15, 'trials': 20}
        
        performance = self.bandit.get_algorithm_performance(algorithm)
        
        assert performance['success_rate'] == 0.75
        assert performance['trials'] == 20
        assert performance['successes'] == 15
        assert 'confidence_interval' in performance


class TestRecommendationEngineIntegration:
    """Test integration between engine components"""
    
    def setup_method(self):
        """Set up test fixtures"""
        self.engine = RecommendationEngine()
    
    @pytest.mark.integration
    def test_complete_recommendation_pipeline(self):
        """Test complete recommendation generation pipeline"""
        # Create a complete test scenario
        scenario = create_recommendation_scenario(num_users=5, recommendations_per_user=2)
        test_user = scenario['users'][0]
        
        with patch('apps.core.recommendation_engine.RecommendationEngine.generate_user_recommendations') as mock_generate:
            mock_generate.return_value = scenario['content_recommendations'][:3]
            
            # Test the pipeline
            recommendations = self.engine.generate_user_recommendations(test_user, limit=5)
            
            assert len(recommendations) <= 5
            assert all(rec.user == test_user for rec in recommendations)
    
    @pytest.mark.integration
    def test_engine_with_real_data(self):
        """Test engine with realistic user data"""
        user = UserFactory()
        profile = UserBehaviorProfileFactory(
            user=user,
            preferred_pages={
                '/dashboard/': 45,
                '/reports/sales/': 30,
                '/assets/list/': 20,
                '/settings/': 5
            },
            preferred_content_types=['dashboard', 'report'],
            exploration_tendency=0.6
        )
        
        # Create similar users
        for i in range(3):
            similar_user = UserFactory()
            similar_profile = UserBehaviorProfileFactory(user=similar_user)
            UserSimilarityFactory(
                user1=user,
                user2=similar_user,
                similarity_score=0.8 - i * 0.1
            )
            
            # Similar users have recommendations
            ContentRecommendationFactory(
                user=similar_user,
                content_type='report',
                relevance_score=0.8
            )
        
        # Generate recommendations
        recommendations = self.engine.generate_user_recommendations(user, limit=10)
        
        # Should generate some recommendations
        assert len(recommendations) > 0
    
    @pytest.mark.performance
    def test_recommendation_generation_performance(self):
        """Test recommendation generation performance"""
        import time
        
        user = UserFactory()
        UserBehaviorProfileFactory(user=user)
        
        # Create many similar users for performance test
        for i in range(50):
            similar_user = UserFactory()
            UserBehaviorProfileFactory(user=similar_user)
            UserSimilarityFactory(user1=user, user2=similar_user, similarity_score=random.uniform(0.3, 0.9))
        
        start_time = time.time()
        recommendations = self.engine.generate_user_recommendations(user, limit=10)
        end_time = time.time()
        
        generation_time = end_time - start_time
        
        # Should complete within reasonable time (adjust threshold as needed)
        assert generation_time < 2.0  # 2 seconds
        assert len(recommendations) <= 10