---
# Prometheus Alerting Rules
#
# Comprehensive alerting for observability metrics.
#
# Observability Enhancement (2025-10-01):
# - GraphQL security alerts (rate limiting, complexity)
# - Celery task health alerts (retries, idempotency)
# - Mutation failure alerts
# - OTEL tracing health alerts
#
# Alert Severity Levels:
# - critical: Immediate action required (page on-call)
# - warning: Needs attention within 15 minutes
# - info: Informational, review during business hours
#
# Integration:
# - Alertmanager for routing and grouping
# - PagerDuty for critical alerts
# - Slack for warning alerts
# - Email for info alerts

groups:
  # ==========================================================================
  # GraphQL Security Alerts
  # ==========================================================================
  - name: graphql_security
    interval: 30s
    rules:
      # High rate of GraphQL rate-limit rejections (potential attack)
      - alert: HighGraphQLRateLimitRejections
        expr: |
          rate(graphql_rate_limit_hits_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
          component: graphql
          category: security
        annotations:
          summary: "High rate of GraphQL rate-limit rejections"
          description: |
            GraphQL endpoint {{ $labels.endpoint }} is experiencing {{ $value | humanize }} rate-limit rejections per second.
            This may indicate:
            - Malicious attack (DoS attempt)
            - Misconfigured client (retry loop)
            - Legitimate traffic spike (review rate limits)

            Check Grafana dashboard: /d/graphql-security/graphql-security
          runbook_url: "https://docs.example.com/runbooks/graphql-rate-limit"

      # Critical: Very high rate-limit rejections (active attack)
      - alert: CriticalGraphQLRateLimitRejections
        expr: |
          rate(graphql_rate_limit_hits_total[5m]) > 50
        for: 1m
        labels:
          severity: critical
          component: graphql
          category: security
        annotations:
          summary: "CRITICAL: GraphQL under active rate-limit attack"
          description: |
            GraphQL endpoint {{ $labels.endpoint }} is experiencing {{ $value | humanize }} rejections/sec.
            This is likely an active DoS attack.

            IMMEDIATE ACTIONS:
            1. Enable IP blocking: Check Grafana for attacking IPs
            2. Increase rate limits temporarily if legitimate traffic
            3. Enable CloudFlare DDoS protection
            4. Review auth logs for compromised credentials
          runbook_url: "https://docs.example.com/runbooks/graphql-dos-attack"

      # High complexity rejection rate (potential attack)
      - alert: HighGraphQLComplexityRejections
        expr: |
          rate(graphql_complexity_rejections_total[5m]) > 5
        for: 2m
        labels:
          severity: warning
          component: graphql
          category: security
        annotations:
          summary: "High rate of GraphQL complexity rejections"
          description: |
            GraphQL endpoint {{ $labels.endpoint }} is rejecting {{ $value | humanize }} queries/sec for {{ $labels.reason }}.

            Possible causes:
            - Complexity bomb attack (deep nesting, circular queries)
            - Legitimate complex query from frontend
            - Misconfigured query complexity limits

            Review rejected queries in logs (correlation_id filter).
          runbook_url: "https://docs.example.com/runbooks/graphql-complexity"

  # ==========================================================================
  # GraphQL Mutation Health Alerts
  # ==========================================================================
  - name: graphql_mutations
    interval: 30s
    rules:
      # High mutation failure rate (system health issue)
      - alert: HighGraphQLMutationFailureRate
        expr: |
          (
            rate(graphql_mutations_total{status="failure"}[5m]) /
            rate(graphql_mutations_total[5m])
          ) > 0.1
        for: 5m
        labels:
          severity: warning
          component: graphql
          category: reliability
        annotations:
          summary: "High GraphQL mutation failure rate"
          description: |
            Mutation {{ $labels.mutation_type }} has {{ $value | humanizePercentage }} failure rate.

            Check:
            - Database connectivity (PostgreSQL locks)
            - Validation errors (check error logs)
            - Service dependencies (external APIs down)
            - Recent deployments (rollback if needed)
          runbook_url: "https://docs.example.com/runbooks/mutation-failures"

      # Slow mutation execution (performance degradation)
      - alert: SlowGraphQLMutationExecution
        expr: |
          histogram_quantile(0.95,
            rate(graphql_mutation_duration_seconds_bucket[5m])
          ) > 3
        for: 5m
        labels:
          severity: warning
          component: graphql
          category: performance
        annotations:
          summary: "GraphQL mutations are slow"
          description: |
            95th percentile mutation execution time is {{ $value | humanizeDuration }}.
            This is above the 3-second SLO.

            Investigate:
            - Database query performance (check slow query log)
            - N+1 query issues (check ORM queries)
            - External API latency (check service dependencies)
          runbook_url: "https://docs.example.com/runbooks/slow-mutations"

  # ==========================================================================
  # Celery Task Health Alerts
  # ==========================================================================
  - name: celery_health
    interval: 30s
    rules:
      # High idempotency dedupe hit rate (duplicate task spam)
      - alert: HighCeleryIdempotencyDuplicates
        expr: |
          (
            rate(celery_idempotency_dedupe_total{result="hit"}[5m]) /
            rate(celery_idempotency_dedupe_total[5m])
          ) > 0.3
        for: 5m
        labels:
          severity: warning
          component: celery
          category: reliability
        annotations:
          summary: "High Celery task duplicate rate"
          description: |
            Task {{ $labels.task_name }} has {{ $value | humanizePercentage }} duplicate submission rate.

            Possible causes:
            - Client retry loop (check frontend logs)
            - Celery Beat schedule overlap (check beat schedule)
            - Race condition in task submission (review code)

            Idempotency is preventing duplicate execution, but investigate root cause.
          runbook_url: "https://docs.example.com/runbooks/celery-duplicates"

      # High task retry rate (systemic failures)
      - alert: HighCeleryTaskRetryRate
        expr: |
          rate(celery_task_retries_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: celery
          category: reliability
        annotations:
          summary: "High Celery task retry rate"
          description: |
            Task {{ $labels.task_name }} is retrying {{ $value | humanize }} times/sec due to {{ $labels.reason }}.

            Check:
            - Database connectivity (check PostgreSQL health)
            - External API availability (check service status)
            - Worker health (check Celery worker logs)
            - Dead letter queue (check DLQ for failed tasks)
          runbook_url: "https://docs.example.com/runbooks/celery-retries"

      # Critical tasks failing repeatedly (data integrity risk)
      - alert: CriticalCeleryTaskFailures
        expr: |
          rate(celery_task_retries_total{task_name=~"auto_close_jobs|ticket_escalation|create_ppm_job"}[5m]) > 1
        for: 2m
        labels:
          severity: critical
          component: celery
          category: reliability
        annotations:
          summary: "CRITICAL: Critical Celery tasks failing"
          description: |
            Critical task {{ $labels.task_name }} is failing with retries ({{ $labels.reason }}).

            IMMEDIATE ACTIONS:
            1. Check worker health: celery -A intelliwiz_config inspect active
            2. Check database: psql -c "SELECT pg_stat_activity"
            3. Check dead letter queue: DLQManager.get_failed_tasks()
            4. Manually trigger task if needed

            This affects business-critical operations!
          runbook_url: "https://docs.example.com/runbooks/critical-task-failures"

  # ==========================================================================
  # OTEL Tracing Health Alerts
  # ==========================================================================
  - name: otel_health
    interval: 30s
    rules:
      # High tracing error rate (observability degradation)
      - alert: HighOTELTracingErrorRate
        expr: |
          rate(otel_trace_errors_total[5m]) > 1
        for: 5m
        labels:
          severity: info
          component: observability
          category: monitoring
        annotations:
          summary: "OTEL tracing errors detected"
          description: |
            OTEL tracing is experiencing {{ $value | humanize }} errors/sec.

            This does not affect application functionality, but reduces observability.

            Check:
            - Jaeger exporter connectivity (check Jaeger host)
            - Span export buffer overflow (check OTEL config)
            - Tracing middleware errors (check app logs)
          runbook_url: "https://docs.example.com/runbooks/otel-errors"

  # ==========================================================================
  # Metrics Export Health Alerts
  # ==========================================================================
  - name: prometheus_health
    interval: 30s
    rules:
      # Prometheus scrape failures (monitoring blind spots)
      - alert: PrometheusScrapeFailing
        expr: |
          up{job="django-app"} == 0
        for: 2m
        labels:
          severity: warning
          component: prometheus
          category: monitoring
        annotations:
          summary: "Prometheus cannot scrape Django app"
          description: |
            Prometheus scrape target {{ $labels.instance }} is down.

            Check:
            - Django app is running: systemctl status django
            - /metrics endpoint is accessible: curl http://localhost:8000/monitoring/metrics/export/
            - Firewall rules (check iptables/security groups)
            - Prometheus IP whitelist (PROMETHEUS_ALLOWED_IPS setting)
          runbook_url: "https://docs.example.com/runbooks/prometheus-scrape"

      # Metrics export endpoint slow (performance issue)
      - alert: SlowPrometheusMetricsExport
        expr: |
          http_request_duration_seconds{endpoint="/monitoring/metrics/export/"} > 0.1
        for: 5m
        labels:
          severity: info
          component: prometheus
          category: performance
        annotations:
          summary: "Prometheus metrics export is slow"
          description: |
            Metrics export endpoint is taking {{ $value | humanizeDuration }} (> 100ms SLO).

            This may indicate:
            - Too many metrics (check metric cardinality)
            - High label cardinality (review label usage)
            - Application under load (check worker CPU/memory)
          runbook_url: "https://docs.example.com/runbooks/slow-metrics-export"

# ==========================================================================
# Alertmanager Configuration Reference
# ==========================================================================
#
# Route these alerts to appropriate channels:
#
# - critical: PagerDuty (immediate paging)
# - warning: Slack #alerts channel (15-minute SLA)
# - info: Email ops@example.com (review during business hours)
#
# Example Alertmanager routes:
#
# route:
#   receiver: 'default'
#   group_by: ['alertname', 'severity']
#   routes:
#     - match:
#         severity: critical
#       receiver: pagerduty
#       group_wait: 10s
#       group_interval: 5m
#     - match:
#         severity: warning
#       receiver: slack
#       group_wait: 30s
#       group_interval: 10m
#     - match:
#         severity: info
#       receiver: email
#       group_wait: 5m
#       group_interval: 1h
#
# ==========================================================================
